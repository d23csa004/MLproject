# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Uc-whPue5s94AJWemR8NTwcBojh-__jZ
"""

import tensorflow as tf
from tensorflow.keras import layers, models
from sklearn.model_selection import train_test_split
from tensorflow.keras.datasets import mnist
import numpy as np

# Load MNIST dataset
(X_train, y_train), (X_test, y_test) = mnist.load_data()

# Preprocess the data
X_train = X_train.reshape(-1, 28, 28, 1).astype('float32') / 255.0  # Reshape and normalize
X_test = X_test.reshape(-1, 28, 28, 1).astype('float32') / 255.0
y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)  # One-hot encode labels
y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)

# Deeper CNN model
deeper_cnn_model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.Flatten(),
    layers.Dense(64, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# Compile deeper CNN model
deeper_cnn_model.compile(optimizer='adam',
                         loss='categorical_crossentropy',
                         metrics=['accuracy'])

# Train deeper CNN model
deeper_cnn_history = deeper_cnn_model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)

# Wider CNN model
wider_cnn_model = models.Sequential([
    layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(128, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Flatten(),
    layers.Dense(256, activation='relu'),
    layers.Dense(10, activation='softmax')
])

# Compile wider CNN model
wider_cnn_model.compile(optimizer='adam',
                        loss='categorical_crossentropy',
                        metrics=['accuracy'])

# Train wider CNN model
wider_cnn_history = wider_cnn_model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)

# CNN model with Dropout
cnn_dropout_model = models.Sequential([
    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    layers.MaxPooling2D((2, 2)),
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),
    layers.Dropout(0.25),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.5),
    layers.Dense(10, activation='softmax')
])

# Compile CNN model with Dropout
cnn_dropout_model.compile(optimizer='adam',
                          loss='categorical_crossentropy',
                          metrics=['accuracy'])

# Train CNN model with Dropout
cnn_dropout_history = cnn_dropout_model.fit(X_train, y_train, epochs=10, batch_size=128, validation_split=0.2)

# Evaluate models
deeper_cnn_loss, deeper_cnn_accuracy = deeper_cnn_model.evaluate(X_test, y_test)
print(f"Deeper CNN Test Accuracy: {deeper_cnn_accuracy}")

wider_cnn_loss, wider_cnn_accuracy = wider_cnn_model.evaluate(X_test, y_test)
print(f"Wider CNN Test Accuracy: {wider_cnn_accuracy}")

cnn_dropout_loss, cnn_dropout_accuracy = cnn_dropout_model.evaluate(X_test, y_test)
print(f"CNN with Dropout Test Accuracy: {cnn_dropout_accuracy}")